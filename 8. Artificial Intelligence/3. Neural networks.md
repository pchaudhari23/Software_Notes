NEURAL NETWORKS:

- Machine Learning & Deep Learning use Neural Networks.
- Machine Learning 1 OR 2 hidden layers in network and supervised learning - The data is structured or labelled by human experts for the network to learn.
- Deep Learning - Many hidden layers - Unsupervised learning.
- More than three layers including input and output layers is Deep network, just two or three layers is basic neural network.
- Neural networks is actually a sub-field of machine learning, and deep learning is a sub-field of neural networks i.e. AI => ML => NN => DL => LLM. And DS.

How it works:

- Neural networks, or artificial neural networks (ANNs), are comprised of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network by that node.
- Deep neural networks consist of multiple layers of interconnected nodes, each building upon the previous layer to refine and optimize the prediction or categorization. This progression of computations through the network is called forward propagation. The input and output layers of a deep neural network are called visible layers. The input layer is where the deep learning model ingests the data for processing, and the output layer is where the final prediction or classification is made.
- Another process called backpropagation uses algorithms, like gradient descent, to calculate errors in predictions and then adjusts the weights and biases of the function by moving backwards through the layers in an effort to train the model. Together, forward propagation and backpropagation allow a neural network to make predictions and correct for any errors accordingly. Over time, the algorithm becomes gradually more accurate. - Adjust the weights on inputs based on trail and error.

Types of NN:

1. Feedforward neural networks, or multi-layer perceptrons (MLPs)
2. CNN
3. RNN

- Nodes, bias, inputs, weights, activation function, input layer, hidden layer, output layer.
- Think of each individual node as its own linear regression model.
  x = ∑wixi + bias = w1x1 + w2x2 + w3x3 + bias
  output y = f(x) = 1 if ∑w1x1 + b>= 0; 0 if ∑w1x1 + b < 0

Activation Functions: f(x) is Activation function.
Types:

- Binary: logistic function
- Linear: line function
- Sigmoid

Features: Input variable to neural network or ML model
Label: Output variable of neural network or ML model

Number of input layer nodes = Number of features
Number of output layer nodes = Type/Purpose of ML algorithm - Regression, classification, clustering etc.
Number of hidden layers and number of hidden layer nodes - Determined by experimentation, fine tuning, requirements, computational power etc.

- Non numeric features are converted into numeric format by encoding.
- Threshold is determined by activation function. If the output is greated than threshold, the neuron is fired and passes data to next layer.

Weights:

- Value assigned to connection between two nodes. It's strength between two nodes.
- It is numerical value multiplied when output from one node is passed as input to another node.

Bias:

- Every node has a bias.
- Bias is not applied to input layer nodes, just hidden and output layers.
- Bias is also updated during training using similar process of weights.

Training:

1. Weights are initially assigned randomly or using some strategy.
2. Forward propogation
3. Loss calculation - Loss function - Deviation from expected outcome.
4. Backward propogation OR Backpropogation
5. Gradient descent - Weights are updated iteratively to minimize loss.

---

Vector:

- It's a 1 dimensional array.
- Input to a particulat network layer is a vector. It is n x 1 vector.
- Every layer also has a bias vector i.e. vector containing bias values of all nodes in that layer.

Matrix:

- It's a 2 dimensional array.
- In a a x b matrix, a is number of rows and b is number of columns.

Weight matrix:

- Weights between nodes are typically represented using a weight matrix.
- Every layer has its own weight matrix.
- It will be a x b matrix where a - number of nodes in current layer, b - number of nodes in previous layer.

Every layer has a bias vector i.e vector containing bias values of all nodes in that layer.

Output vector of a layer = (Input vector to that layer x Weight matrix of that layer) + Bias vector of that layer

---
