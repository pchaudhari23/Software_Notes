LARGE LANGUAGE MODEL:

- Generative AI applications are powered by large language models (LLMs).
- Statistical algorithm that predicts the most probable sequence of words.
- LLM are sub sets of deep learning.
- Transformer models are trained with large volumes of text, enabling them to represent the semantic relationships between words and use those relationships to determine probable sequences of text that make sense.
- Transformer model architecture consists of two components, or blocks: Encoder + Decoder.
- An encoder block that creates semantic representations of the training vocabulary.
- A decoder block that generates new language sequences.
- Bidirectional Encoder Representations from Transformers (BERT) model developed by Google to support their search engine uses only the encoder block, while the Generative Pretrained Transformer (GPT) model developed by OpenAI uses only the decoder block.

How it works:

1. Prompt : Text input given to LLM
2. Tokenization: The first step in training a transformer model is to decompose the training text into tokens - in other words, identify each unique text value.
3. Embeddings: To create a vocabulary that encapsulates semantic relationships between the tokens, we define contextual vectors, known as embeddings, for them.
4. Attention layer in encoder and decoder.

- Derive semantic meaning from corpus (text body).
- Break words in the corpus and assign every word, partial word, combination of words and punctuations a token/number.
- Same token is used for repeated words.
- Tokenization - we get an array of tokens
- Text processing before tokenization: Text normalisation, stop word removal, n-grams, stemming, Frequency analysis: to get general meaning or crux of the text by checking repeatative words.
- Embeddings: encoding of language tokens as vectors (multi-valued arrays of numbers). Every token in the sentence is assigned as vector - multi valued array.
- Assign every word a 3D vector - an array of x y and z coordinate in space.
- The vector represents a co-ordinate in 3D space - [x,y,z]
- Some tokens are closer to one another in a dimension so they are more semantically related. Eg: dog[10,y,z], cat[10,y,z], rat[10,y,z] all might have same value in x co-ordinate so they are animals.
- Tokens that are commonly used in the same context are closer together dimensionally than unrelated words.
- Confidence score - between 0 to 1
- In complex LLMs - Vectors are more dimension than 3D.

Applications of LLMs:

1. Text analysis
2. Sentiment analysis - positive and negative
3. Machine translation
4. Summarisation
5. Conversational AI

---

Steps in training a LLM:

1. Data collection
2. Preprocessing
3. Tokenization
4. Model Architecture
5. Training
6. Evaluation
7. Fine Tuning
8. Testing
9. Deployment
10. Publishing
11. Monitoring
12. Security

---

- GENERATIVE AI: Getting trained on massive information and generating content which is similar to the data the model was trained on. Making the content human like.

  1. Text/chat generation - ChatGPT
  2. Code generation - OpenAI Codex
  3. Image generation - DALLÂ·E
  4. Voice

- Chatbots
- Natural Language Processing (NLP)
- Large language model (LLM):
  GPT: Generative Pre-trained Transformer

Google: Gemini and Bard
Microsoft: Co-pilot
Open AI: ChatGPT

- GPT-3, BERT, or DALL-E 2

Discriminative AI model: Used to discriminate between the inputs based on training data.

---

How to consume enterprise ready LLMs into our software applications?
Building an RAG solution?
