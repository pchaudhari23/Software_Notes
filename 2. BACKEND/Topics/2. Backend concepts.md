1. Rate limiting : A way to control the number of requests a client (like a user, app or system) can make to a server within a specific period.
2. Load balancing : A way to distribute incoming requests or traffic across multiple servers to ensure no single server gets overloaded.
3. Auto Scaling: A mechanism that automatically increases or decreases the number of server instances based on traffic or resource usage (CPU, memory, requests).
4. Caching : A way to store frequently accesses data temporarily so that it can be retrieved quickly without repeatedly fetching it from original source.
5. CDN : A network of servers distributed across different locations that work together to deliver content to users more quickly.
6. Microservices : An architectural style where an application is divided into smaller, independent services, each responsible for a specific piece of functionality.
7. API Gateway : A server that acts as an entry point for all client requests to your system. It routes those request to the appropriate microservice, handles load balancing and other tasks.
8. Webhooks : A way for one application to send real-time updates or notifications to another application when a specific event happens.
9. Sharding : Method of splitting large database into smaller, more manageble pieces called shards.
10. Proxy : Server that acts as an intermediary between client (like browser) and another server (like a website).
11. Message Queues : System used to send, store and retrieve messages between different parts of an application (or between applications) in a reliable and organized way.
12. Backend consists of a physical server hardware instance and server software like nginx, apache, tomcat, express, dango, IIS etc
13. Contract API/ Mock API: A simulated API that returns predefined responses, used for development and testing before the real backend is ready.
14. API Design doc: A detailed specification describing API endpoints, request/response formats, authentication, error handling, and usage guidelines.
